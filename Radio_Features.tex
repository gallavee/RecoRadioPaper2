%\GL{'Contextual features' does not mean anything to me I prefer 'Useful Features for Encoding Context'}
%\GL{In writing this section I believe we should speak to the potential of what is possible with the various sources of information that we have in our possession rather than what we actually used (a subset).THis is similar to our inclusion of the gamma priors over the precision params. This was my approach in rewriting this section.}
The model as described in the previous section makes no assumptions on the nature of the contextual features beyond the fact that they encode relevant information for choosing the next track to be added to the playlist.  
Since the main contribution of this paper is in the definition of the model and corresponding learning algorithm, our efforts to find the best features for the application of playlist generation are by no means exhaustive. 
However, in this section we offer some insights into the types of features used by the algorithm.


%Each of the feature groups above is used to compute pairwise similarities comparing a candidate track to be added to the list with the previous tracks already in the playlist as well as with similarities to the seed artist. 
In general, the features encode different types of similarities or relations  comparing a candidate track to be added to the playlist with tracks already selected as well as with the seed artist. In cases where specific similarities are not applicable we apply zero-imputation.
\GLw{Added following ulrich's comment:} In our practice the number of features, denoted $d$ below, is on the order of 10-20. We divide these features into four groups categorized according to the type of signal employed in their calculation:
\GLw{Added more formal description of features}

\noindent{\bf Acoustic Audio Features (AAF)} -
Let $\mathcal{D}_a=\left\{\mathbf{x_i} \in \mathbb{R}^d\right\}_{i=1}^{N_a}$ denote the audio samples (encoded as a vector of continuous features) for a particular artist $a$. Let $\hat{\theta}_a=\argmax_{\theta} \prod_i p\left(x_i\mid\theta\right)$ denote the maximum likelihood parameter setting for the (Gaussian Mixture Model) GMM  for the distribution of acoustic audio features. The audio similarity between two artists $a_1$ and $a_2$ is then given by $\textsc{KL}\left[ p\left(\mathbf{x}\mid\theta_{a_1}\right) \mid \mid p\left(\mathbf{x}\mid\theta_{a_2}\right)\right
])$, the Kullback Liebler divergence between the two corresponding distributions. To compute the features $\mathbf{x}_i$, we compute the mel frequency cepstrum coefficients (MFCC). This approach follows~\cite{ReynoldsQD00} and can be applied similarly to compute the similarity between audio tracks.

\noindent{\bf Usage Features (UF)} -
Following \cite{xbox-www} we learn a low-rank factorization of the matrix $\mathcal{R}$,
 where $r_{i,j}$ the element on the $i$-th row and $j$-th column denotes the binary rating given to the $j$-th artist in the catalog by the $i$-th user of the system. This formulation is parameterized by a $k$-dimensional vector for each user and artist appearing in the training data. More precisely, the user vectors $\left\{\mathbf{u}_i\right\}_{i=1}^{|U|}$ and artist vectors $\left\{\mathbf{a}_j\right\}_{j=1}^{|A|}$, collectively denoted $\theta_u$ are chosen to optimize the Bayesian model $p(\mathcal{D}_u,\theta_u)$ (Equation (5) in \cite{xbox-www}) over the training dataset containing binary interactions between users and artists, denoted $\mathcal{D}_u$.  The similarity between two artists $a_1$ and $a_2$ is then given by $\sigma \left(\mathbf{a}_{a_1}^\top\mathbf{a}_{a_2}\right)$, where $\sigma$ denotes the sigmoid function. A similar treatment can be applied to a matrix of user-track interaction and used to derive similarities between tracks.

\noindent{\bf Meta-Data Features (MDF)} -
Meta-Data features are based on the semantic tags associated with different tracks and artists in the catalog(e.g. ``easy-listening'', ``upbeat'', ``90's''). We obtain this information from a third-party dataset. Each artist $j$ in the catalog is  encoded as a vector $\mathbf{b}_j \in \left\{0,1\right\}^{|V|}$, where $V$ denotes the set of  possible binary semantic tags. These vectors are generally sparse, as each artist corresponds to only a small number of semantic tags.  The similarity between two artists $a_1$ and $a_2$ is then given by the cosine similarity: 
$$\frac{\mathbf{b}_{a_1}^\top\mathbf{b}_{a_2}}{\parallel\mathbf{b}_{a_1}\parallel\parallel\mathbf{b}_{a_2}\parallel}$$
Track to track similarity can be computed by using sparse vectors representing tracks.

\noindent{\bf Popularity Features (PF)} -
Popularity is a measure of the prevalence of a track or artist in the dataset.
Popularity is used to compute unary features representing the popularity of a candidate track and its artist. For a particular artist $a_1$ such a feature is computed $\textsc{pop}_{a1}=\frac{\#n_{\mathcal{D}_u}\left(a_1\right)}{|\mathcal{D}_u|}$, where $\#n_{\mathcal{D}_u}\left(a_1\right)$ denotes the number of users who consumed a track by artist $a_1$  in the training corpus $\mathcal{D}_u$. Pairwise popularity features are computed relating the popularity of the candidate track and its artist to the popularity of the seed artist and previous tracks. The relative popularity of artists $a_1$ and $a_2$ can be computed as $\frac{\textsc{pop}_{a2}}{\textsc{pop}_{a1}}$. track to track and track to artist relative popularity can be computed similarly.


%The above features are computed for the various signal sources, different taxonomy level (i.e. artist or track) and different aggregation levels (e.g. averaged over all previous tracks vs only last track) allows for many features. Then, forward feature selection is performed in order to remove redundant features according to the prediction task at hand.





%
%We base our features on three types of resources. The first one is the audio itself. The Audio Based Similarity (ABS) system maps audio tracks to a vector space. Specifically, for each track we extract 12 MFCCs augmented by delta and delta-delta coefficients to form a frame representation feature vector of dimensionality 36. We use a frame size of 2048 with half overlapping frames \SBE{perhaps translate to stride?}. Then, we learn a background distribution from a subset of the entire dataset. To this end, a background GMM with 256 components is trained by applying Expectation Maximization (EM) \cite{ReynoldsQD00}. Given a set of MFCCs features that are extracted from a specific track, we first apply MAP adaptation \cite{ReynoldsQD00}, where the background GMM distribution is taken as a prior. The final representation is determined from the adapted GMM concatenated means. Therefore, the final representation for each audio track is a 9216 dimensional vector. Using ABS we extract artist to artist, $a2a_{ABS}$, and track to track similarity, $t2t_{ABS}$. These features perceive the audio similarity, but they lack semantics such as popularity and origin. The second resource is the usage we collect from the groove client. Using matrix factorization \SBE{Ref RecoMF?} we extract $a2a_{MF}$ and $t2t_{MF}$ similarity based on usage patterns. From the usage we also compute the track popularity $t_{pop}$ and artist popularity $a_{pop}$. In addition, from manually curated meta-data on artist's genres, we compute genre similarity, $a2a_{g}$, between artists by measuring the genre distance between the artists \SBE{Unclear how we compute this, what is genre distance? Is this Jaccard on the label set?}. 
%
%With these three feature types we provide features for computing the music similarity of tracks from both aspects, the audio content of the track and the social-usage context. 
%
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
% \hline
% \diaghead{\theadfont Diag ColumnmnHead II}%
%{Features}{Resource type}&
%\thead{Audio}&\thead{Usage}&\thead{Content}\\
% \hline
% Track to track similarity & \checkmark & \checkmark & \\ 
% \hline
% Artist to artist similarity & \checkmark & \checkmark & \checkmark\\ 
% \hline
% Track popularity & & \checkmark & \\ 
% \hline
% Artist popularity & & \checkmark & \\ 
% \hline
%\end{tabular}
%    \caption{This table shows features we compute from each of the three resources.}
%\end{center}
%\end{table}
%
%To summarize, our feature vector $\x$ is composed of the set of features $\{a2a_{ABS},t2t_{ABS},a2a_{MF},t2t_{MF},a2a_{g},t_{pop},a_{pop}\}$ computed on the context seed artist, current track and candidate track. \SBE{Need to verify that this is the complete feature set and elaborate on relation of features to state}
%
